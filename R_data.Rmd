---
title: "tidycensus"
output:
  pdf_document: default
  html_document: default
---

```{r setup}
library(knitr)
library(readr)
library(dplyr)
library(ggplot2)
library(rworldmap)
library(usmap)
library(tidyr)
library(tidycensus)
library(mapdeck)
#library(zillowR)
library(fredr)
library(stringr)
#library(ggmap)
#library(revgeo)
library(RCurl)
library(RJSONIO)
library(tidyquant)
fred_key <- '804592a605e2f219fea048cb93408683'
fredr_set_key('804592a605e2f219fea048cb93408683')
opts_chunk$set(fig.align='center', dpi=100, message=FALSE, warning=FALSE, cache=TRUE)
output <- opts_knit$get("rmarkdown.pandoc.to")
if(!is.null(output)) {
if (output=="html") opts_chunk$set(out.width = '400px') else
  opts_chunk$set(out.width='.6\\linewidth')
}
```

## Get Data and do basic Transformation
use this for the k_e_y: 335e6ba6cab417fe8b2c91f62c337e4307e2912f
```{r }
a <- get_flows(
   geography = "metropolitan statistical area",
   variables = c("POP1YR", "POP1YRAGO"),
   geometry = TRUE,
   output = "wide",
   show_call = TRUE,
   key = '',
  )

b <- data.frame(unique(a$FULL1_NAME))
colnames(b) <- c('col1')
c <- data.frame(do.call('rbind', strsplit(as.character(b$col1),',')))
colnames(c) <- c('city','state_plus')
d <- data.frame(do.call('rbind', strsplit(as.character(c$state_plus),' ')))
colnames(d) <- c('state','etc')
fin <- cbind(d$etc,c,b)
colnames(fin) <- c('state','city','state_plus','FULL1_NAME')
a_new <- merge(x = a, y = fin, by = "FULL1_NAME", all.x = TRUE)

write.csv(a_new,'new_data.csv')

#the above ends up in 8.4 Gb in the environment which makes it quite large, thus, breaking this chunk up and exporting to csv.
```


```{r}
#Transform to get lat, lng and zip
a_new <- read_csv('new_data.csv')
head(a_new)

#convert centroid to zip
a_new[c('lng', 'lat')] <- str_split_fixed(a_new$centroid2, ',', 2)
a_new$lng = substring(a_new$lng, 3)
a_new$lat = str_sub(a_new$lat, 1, 16)
unique(a_new$lng1)
head(a_new) 


#didnt work
#start <- Sys.time()
#This line do all the reverse geocoding using Photon as a provider
#results<-revgeo(longitude=a_new$lng, 
#                            latitude=a_new$lat, 
#                            provider = 'photon', output="frame")
#end <- Sys.time()
#head(results)



#didn't work
#latlon2zip <- function(lat, lon) {
#    url <- sprintf("http://nominatim.openstreetmap.org/reverse?format=json&lat=%f&lon=%f&zoom=18&address#details=1", lat, lon)
#    res <- fromJSON(url)
#    return(res[["address"]][["postcode"]])
#}
#names(a_new)
#a_new %>%  mutate(zp = latlon2zip(lat=lat, lon=lng))


#write.csv(a_new,'new_data.csv')

```

```{r}
#df2 <- tidyquant::tq_get("HOUST",get="economic.data",from="1900-01-01")


```

```{r}
hi_dc_flows <- get_flows(
  state = c('ID','UT'),
  geography = "county", #metropolitan statistical area",
  #breakdown = "RACE", #race labels can only be used for Year < 2016
  #breakdown_labels = TRUE,
  #msa = 38060,
  #year = 2019, #only <= 2019
  geometry = TRUE,
  key = '335e6ba6cab417fe8b2c91f62c337e4307e2912f'
  )

ID_move_in <- hi_dc_flows %>% filter(FULL1_NAME %like% 'Idaho') %>%
  filter(!is.na(GEOID2), variable == "MOVEDIN") %>% 
  slice_max(n = 50, order_by = estimate) %>% 
  mutate(
    width = estimate / 500,
    tooltip = paste0(
      scales::comma(estimate * 5, 1),
      " people moved from ", str_remove(FULL2_NAME, "Metro Area"),
      " to ", str_remove(FULL1_NAME, "Metro Area"), " between 2014 and 2019"
      )
    )

UT_move_in <- hi_dc_flows %>% filter(FULL1_NAME %like% 'Utah') %>%
  filter(!is.na(GEOID2), variable == "MOVEDIN") %>% 
  slice_max(n = 50, order_by = estimate) %>% 
  mutate(
    width = estimate / 500,
    tooltip = paste0(
      scales::comma(estimate * 5, 1),
      " people moved from ", str_remove(FULL2_NAME, "Metro Area"),
      " to ", str_remove(FULL1_NAME, "Metro Area"), " between 2014 and 2019"
      )
    )

ga_ct_flows <- get_flows(
  state = c('MS','WV'),
  geography = "county", #metropolitan statistical area",
  #breakdown = "RACE", #race labels can only be used for Year < 2016
  #breakdown_labels = TRUE,
  #msa = 38060,
  #year = 2019, #only <= 2019
  geometry = TRUE,
  key = '335e6ba6cab417fe8b2c91f62c337e4307e2912f'
  )

MS_move_out <- ga_ct_flows %>% filter(FULL1_NAME %like% 'Mississippi') %>% 
  filter( variable == "MOVEDOUT") %>% 
  slice_max(n = 50, order_by = estimate) %>% 
  mutate(
    width = estimate / 500,
    tooltip = paste0(
      scales::comma(estimate * 5, 1),
      " people moved to ", str_remove(FULL2_NAME, "Metro Area"),
      " from ", str_remove(FULL1_NAME, "Metro Area"), " between 2014 and 2019"
      )
    )

WV_move_out <- ga_ct_flows %>% filter(FULL1_NAME %like% 'West Virginia') %>%
  filter(!is.na(GEOID2), variable == "MOVEDOUT") %>% 
  slice_max(n = 50, order_by = estimate) %>% 
  mutate(
    width = estimate / 500,
    tooltip = paste0(
      scales::comma(estimate * 5, 1),
      " people moved to ", str_remove(FULL2_NAME, "Metro Area"),
      " from ", str_remove(FULL1_NAME, "Metro Area"), " between 2014 and 2019"
      )
    )

 mapdeck(style = mapdeck_style("light"), pitch = 75, token='pk.eyJ1IjoiZGpzdGVhcm5zNDAyIiwiYSI6ImNsMHpkaW0wbDI5bjgzZHBudDZmeGR5c3YifQ.IhJPEXwWZbIyn1imJghNkg') %>% 
  add_arc(
    data = WV_move_out,
    origin = "centroid1",
    destination = "centroid2",
    stroke_width = "width",
    auto_highlight = TRUE,
    stroke_from = "#fc0303",
    tooltip = "tooltip",
    layer_id ="1"
  )  %>%
    add_arc(
    data = MS_move_out,
    origin = "centroid1",
    destination = "centroid2",
    stroke_width = "width",
    auto_highlight = TRUE,
    stroke_from = "#fc0303",
    tooltip = "tooltip",
    layer_id = "2"
  )  %>%
   add_arc(
    data = ID_move_in,
    origin = "centroid1",
    destination = "centroid2",
    stroke_width = "width",
    auto_highlight = TRUE,
    stroke_from = "#00a30b",
    tooltip = "tooltip",
    layer_id = "3"
  ) %>%
    add_arc(
    data = UT_move_in,
    origin = "centroid1",
    destination = "centroid2",
    stroke_width = "width",
    auto_highlight = TRUE,
    stroke_from = "#00a30b",
    tooltip = "tooltip",
    layer_id = "4"
  )
```

```{r}
#delte me!
write.csv(top_move_in,'output.csv')

set_token( read.dcf("~/Documents/.googleAPI", fields = "MAPBOX"))
df <- read.csv(paste0(
  'https://raw.githubusercontent.com/uber-common/deck.gl-data/master/examples/'
  , '3d-heatmap/heatmap-data.csv'
))

df <- df[!is.na(df$lng), ]

library(geojsonsf)
sf <- geojson_sf("https://symbolixau.github.io/data/geojson/SA2_2016_VIC.json")
sf$e <- sf$AREASQKM16 * 10

class(us_val$state.center)

?sf::st_geometry

head(sf)
library(tidygeocoder)  
st <- cbind(state.abb, state.center)
cap <- read_csv('state_capitals.csv')
cap$city_state <- str_c(cap$Capital,", ",cap$State)
us_val <- merge(x=ndp, y=st, by.x=c("State"), by.y=c("state.abb"), all.x=TRUE) 

try <- cap %>% geocode(city_state, method = "arcgis")

try_lls <- try %>% select("Abr.", "lat", "long")

library(tigris)
st <- states() 

sf <- geojson_sf("https://symbolixau.github.io/data/geojson/SA2_2016_VIC.json")
sf$e <- sf$AREASQKM16 * 10
class(sf$geometry)
class(f_us$geo)
st_1 <- st %>% select("NAME","geometry") 
colnames(st_1) <- c('state_name','geometry')
can_new <- try %>% select("State", "Abr.","lat","long")
names(f_us)

try1 <- merge(x=can_new, y=st_1, by.x=c("State"), by.y=c("state_name"))
names(try1)
f_us <- merge(x=try1, y=us_val, by.x=c("Abr."), by.y=c("State"))
f_us <- f_us %>% filter(f_us$State == "Alabama")
count(f_us)
#f_us <- f_us %>% select("lat","long")
  mapdeck(style = mapdeck_style("light"), pitch = 45, token='pk.eyJ1IjoiZGpzdGVhcm5zNDAyIiwiYSI6ImNsMHpkaW0wbDI5bjgzZHBudDZmeGR5c3YifQ.IhJPEXwWZbIyn1imJghNkg') %>% 
  add_arc(
    data = top_move_in,
    origin = "centroid1",
    destination = "centroid2",
    stroke_width = "width",
    auto_highlight = TRUE,
    highlight_colour = "#8c43facc",
    tooltip = "tooltip"
  ) %>%
 add_polygon(
    data = f_us
    , layer = "polygon_layer"
    , elevation = 10000
  )

#sPDF <- joinCountryData2Map( a_new , joinCode = "NAME" , nameJoinColumn = "state" )
#par(mai=c(0,0,0.2,0),xaxs="i",yaxs="i")
#mapCountryData( sPDF, nameColumnToPlot="MOVEDIN" )
#do.call( addMapLegend, c(mapParams, legendWidth=0.5, legendMar = 2))



#Plot 5.1 - basically useless
#plot_usmap(data = a_new, values = "MOVEDIN", color = "red") + 
#  scale_fill_continuous(name = "Population (2015)", label = scales::comma) + 
#  theme(legend.position = "right")
```

```{r}

```
### Include other data?
```{r}

hp <- read_csv('House_price_multifeatures.csv')
head(hp)

plot_usmap(data = a_new, values = "MOVEDIN", color = "red") + 
  scale_fill_continuous(name = "Population (2015)", label = scales::comma) + 
  theme(legend.position = "right")

unique(hp$OwnerState)

#what about FRED data: https://fred.stlouisfed.org/series/MEDDAYONMARUS or https://fred.stlouisfed.org/series/ACTLISCOUUS

fredr(
  series_id = "MEDDAYONMARUS",
  observation_start = as.Date("2019-01-01"),
  observation_end = as.Date("2020-01-01"),
  frequency = "m", # quarterly
  units = "chg" # change over previous value
)


```

```{r}
library(reshape2)
city_house_data <- read_csv('City_zhvi_uc_sfr_tier_0.33_0.67_sm_sa_month.csv')
mdat <- melt(city_house_data, 
             id = c("RegionID",   "SizeRank",   "RegionName", "RegionType" ,"StateName","State","Metro","CountyName"))
head(mdat)
new_data <- city_house_data %>% select("RegionID", "SizeRank", "RegionName", "RegionType", "StateName", "State",  "Metro",  "CountyName","2019-01-31","2019-12-31")

new_data$diff = new_data$`2019-12-31` - new_data$`2019-01-31`

new_data_part = new_data %>% select("State", "diff")
ndp <- new_data_part[!is.na(as.numeric(as.character(new_data_part$diff))),]
ndp = ndp %>% group_by(State) %>% summarise(tot = sum(diff))

write.csv(mdat,'new_city_data.csv')
```

```{r}
zillow <- read_csv('new_city_data.csv')
names(zillow)

z19 <- zillow %>% filter(variable=="2014-01-31" | variable=="2019-12-31")
unique(zillow$variable)
#merge two datasets
```